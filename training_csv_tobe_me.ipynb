{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8J3MxUg0Tm"
      },
      "source": [
        "# Training the Gemini Model with your text\n",
        "\n",
        "First configure the Colab can access your drive folder where you put hte CSV file with your text;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8dhbUGKaf6V",
        "outputId": "64b6ae49-a6d1-4bd6-c82f-be802f484462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJimLN3YhOTP"
      },
      "source": [
        "Read the csv file and transform in the pandas data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4jp6Z5TQaf6W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "msgs = pd.read_csv('/content/drive/MyDrive/COLAB/msg.csv',delimiter=\"|\")\n",
        "\n",
        "msgs.head(1)\n",
        "msg_dataset = [row[\"txt\"] for index,row in msgs.iterrows()]\n",
        "len(msg_dataset)\n",
        "del msgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkSNNDCphXXz"
      },
      "source": [
        "Install the Keras, Tensorflow and keras-nlp to easely ge the Gemini Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VpqTjEV2bNl_"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "\n",
        "print(keras.__version__ , keras_nlp.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_N5yLWpWlH",
        "outputId": "e62f2f36-457f-4596-8c19-d9c7bf67cfe7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1.1 0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj68Twkthil1"
      },
      "source": [
        "Set up the **KAGGLE_KEY** and **KAGGLE_USERNAME** to download the model .\n",
        "\n",
        "You can do the process hard, download the model file and configure the tensorflow/keras to fit with your text ([hard way](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)). But has a easy way and I use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QjT9o_gZgmtu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"KAGGLE_KEY\"]= userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"]= userdata.get('KAGGLE_USERNAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVyPQIwxiqzL"
      },
      "source": [
        "Finally ...\n",
        "\n",
        "Get the Gemini model and fine turn with your text. after teste the model again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D22i04Xcaf6W",
        "outputId": "e95f3849-598d-4b9b-ecf0-2a5ec76bc483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.22 s, sys: 15.3 s, total: 24.5 s\n",
            "Wall time: 34.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.backbone.enable_lora(rank=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(gemma_lm.generate(\"qual o sentido da vida?\", max_length=64))"
      ],
      "metadata": {
        "id": "DY8K553Uyn9e",
        "outputId": "f2e2c9a2-c263-492a-8035-2120a01b7bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qual o sentido da vida?\n",
            "A vida é uma coisa que não tem sentido, mas que é muito importante.\n",
            "A vida é uma coisa que não tem sentido, mas que é muito importante.\n",
            "A vida é uma coisa que não tem sentido, mas que é muito importante.\n",
            "A vida é uma coisa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.fit(msg_dataset, epochs=1, batch_size=1)"
      ],
      "metadata": {
        "id": "CNk1uYCJyd6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GytsIrgiuQv"
      },
      "outputs": [],
      "source": [
        "print(gemma_lm.generate(\"qual o sentido da vida?\", max_length=64))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}